
```{r}
library(tidyverse)
library(tidymodels)
library(rpart) #for classification trees
library(rpart.plot) #plotting trees
library(RColorBrewer) #better visualization of classification trees
library(rattle) #better visualization of classification trees
library(caret) #for easy confusion matrix creation 
```
```{r}
heart <- read.csv("heart_disease-1.csv")
```

```{r}
heart = heart%>% mutate(Sex = as_factor(Sex)) %>%
  mutate(ChestPainType = as_factor(ChestPainType))%>%
  mutate(RestingECG = as_factor(RestingECG))%>%
  mutate(ExerciseAngina = as_factor(ExerciseAngina))%>%
  mutate(ST_Slope = as_factor(ST_Slope))%>%
  mutate(HeartDisease = as_factor(HeartDisease))%>%
  mutate(HeartDisease = fct_recode(HeartDisease, "No" = "0", "Yes" = "1"))
  
```

```{r}
set.seed(12345) 
heart_split = initial_split(heart, prop = 0.7, strata = HeartDisease) #70% in training
train = training(heart_split)
test = testing(heart_split)
```
Question 1: Split the data into training and testing sets. Your training set should have 70% of the data.
Use a random number (set.seed) of 12345. Stratify your split by the response variable “HeartDisease”.
How many rows are in the training set? 642

```{r}
heart_recipe = recipe(HeartDisease  ~., train)

tree_model = decision_tree() %>% 
  set_engine("rpart", model = TRUE) %>% #don't forget the model = TRUE flag
  set_mode("classification")

heart_wflow = 
  workflow() %>% 
  add_model(tree_model) %>% 
  add_recipe(heart_recipe)

heart_fit = fit(heart_wflow, train)
```

```{r}
tree = heart_fit %>% 
  pull_workflow_fit() %>% 
  pluck("fit")

#plot the tree
fancyRpartPlot(tree)
```

Question 2: Create a classification tree to predict “violator” in the training set (using all of the other
variables as predictors). Plot the tree. You do not need to manually tune the complexity parameter (i.e., it’s
OK to allow R to try different cp values on its own). Do not use k-folds at this point.
The first split in the tree is a split on which variable? B. St_Slope

```{r}
heart_fit$fit$fit$fit$cptable
```
Which cp value is optimal (recall that the optimal cp corresponds to the minimized “xerror” value)? Report
your answer to two decimal places. 0.01742160 
```{r}
set.seed(123)
folds = vfold_cv(train, v = 5)
```

```{r}
heart_recipe2 = recipe(HeartDisease ~., train) %>%
  step_dummy(all_nominal(),-all_outcomes())

tree_model2 = decision_tree(cost_complexity = tune()) %>% 
  set_engine("rpart", model = TRUE) %>% #don't forget the model = TRUE flag
  set_mode("classification")

tree_grid = grid_regular(cost_complexity(),
                          levels = 25) #try 25 sensible values for cp

heart_wflow2 = 
  workflow() %>% 
  add_model(tree_model2) %>% 
  add_recipe(heart_recipe2)

tree_res = 
 heart_wflow2 %>% 
  tune_grid(
    resamples = folds,
    grid = tree_grid
    )

tree_res
```

```{r}
tree_res %>%
  collect_metrics() %>%
  ggplot(aes(cost_complexity, mean)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) 
```
```{r}
best_tree = tree_res %>%
  select_best("accuracy")

best_tree
```

Q4. .85
q5. .0075

```{r}
final_wf = 
  heart_wflow2 %>% 
  finalize_workflow(best_tree)
```

```{r}
final_fit = fit(final_wf, train)

tree = final_fit %>% 
  pull_workflow_fit() %>% 
  pluck("fit")

fancyRpartPlot(tree, tweak = 1) 
```

```{r}
treepred = predict(final_fit, train, type = "class")
head(treepred)
```
```{r}
confusionMatrix(treepred$.pred_class,train$HeartDisease,positive="Yes")
```
.8449
.8609
0.553 

```{r}
treepred2 = predict(final_fit, test, type = "class")
head(treepred)
```
```{r}
confusionMatrix(treepred2$.pred_class,test$HeartDisease,positive="Yes")
```

: 0.8478  